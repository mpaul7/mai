
def tltrain(log_dir, experiment_name, run_name_a, run_name_b, run_name_c, params_a, params_b, params_o, params_c, dataset, valdata):
    """Trnsfer learning
    TODO: somehow tensorboard doesn't work, need to debug why log directory doesn't work
    """
    from tensorflow.keras.models import Model
    from tensorflow.keras import models, layers, metrics
    from tw.cli.params import KERAS_INITIALIZER

    experiment = mlflow.set_experiment(experiment_name)

    combined_params = ExperimentParameters.from_json(params_c)

    params_a = ExperimentParameters.from_json(params_a)

    params_b = ExperimentParameters.from_json(params_b)

    params_o = ExperimentParameters.from_json(params_o)

    def _load_model(run_name, params):               
        model = params.model_class()

        runs = mlflow.search_runs([experiment.experiment_id], filter_string=f"run_name='{run_name}'")
        if len(runs) == 0:
            click.echo(f"Cannot find run with name {run_name} in experiment {experiment_name}")
            return

        run_id = runs.iloc[0]["run_id"]

        artifact_uri = runs.iloc[0]["artifact_uri"]
        if Path(urlparse(artifact_uri + "/" + run_name).path).exists():
            model_uri = artifact_uri + "/" + run_name
        else:
            model_uri = artifact_uri + "/model/data/model.h5"  # for the models generated by HPsearch

        model.load(model_uri, params)
        return model

    pretrained_model_a = _load_model(run_name_a, params_a)
    pretrained_model_b = _load_model(run_name_b, params_b)
    pretrained_model_c = _load_model(run_name_c, params_o)

    print("Model A Layers:")
    for i, layer in enumerate(pretrained_model_a.model.layers):
        print(f"Layer {i}: {layer.name} ({layer.__class__.__name__})")

    print("\nModel B Layers:")
    for i, layer in enumerate(pretrained_model_b.model.layers):
        print(f"Layer {i}: {layer.name} ({layer.__class__.__name__})")

    print("\nModel C Layers:")
    for i, layer in enumerate(pretrained_model_c.model.layers):
        print(f"Layer {i}: {layer.name} ({layer.__class__.__name__})")
    

    # # Define the partial layers for model a and model b
    # # for app level model, mlp first, lstm second, cnn third
    partial_model_a = Model(inputs=pretrained_model_a.model.input,
                            outputs=pretrained_model_a.model.layers[45].output)  # right before concatenate
    partial_model_b = Model(inputs=pretrained_model_b.model.input,
                            outputs=pretrained_model_b.model.layers[20].output)  # right before concatenate
    partial_model_c = Model(inputs=pretrained_model_c.model.input,
                            outputs=pretrained_model_c.model.layers[35].output)  # right before concatenate

    # Freeze all layers in partial model A
    for layer in partial_model_a.layers:
        if not isinstance(layer, layers.InputLayer):
            layer._name = layer.name + '_a'
        layer.trainable = False

    # Freeze all layers in partial model B
    for layer in partial_model_b.layers:
        if not isinstance(layer, layers.InputLayer):
            layer._name = layer.name + '_b'
        layer.trainable = False

    # Freeze all layers in partial model C
    for layer in partial_model_c.layers:
        if not isinstance(layer, layers.InputLayer):
            layer._name = layer.name + '_c'
        layer.trainable = False

    # Define the input shape for both models
    input_A = partial_model_a.input
    input_B = partial_model_b.input
    input_C = partial_model_c.input

    # Extract features from the partial models
    features_A = partial_model_a.output
    features_B = partial_model_b.output
    features_C = partial_model_c.output

    # Concatenate the features extracted from both models
    combined_features = layers.concatenate([features_A, features_B, features_C])

    # Classifier part
    initializer = KERAS_INITIALIZER.get(combined_params.initializer)()

    x = layers.Dropout(combined_params.encoder_dense_dropout_rate)(combined_features)

    for i in range(combined_params.num_encoder_dense):
        x = layers.Dense(units=combined_params.encoder_dense_units_list[i])(x)
        # x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU()(x)

    encoding = layers.Dense(combined_params.openset_encoding_size, activation=None, kernel_initializer=initializer,
                            name='encoding')(x)
    x = layers.LeakyReLU()(encoding)

    for i in range(combined_params.num_final_dense):
        x = layers.Dense(units=combined_params.final_dense_units_list[i], kernel_initializer=initializer)(x)
        # x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU()(x)
        x = layers.Dropout(combined_params.final_dense_dropout_rate)(x)

    score = layers.Lambda(lambda x: tf.math.sqrt(tf.math.reduce_sum(tf.math.square(x), axis=1)),
                          name='unknown_score', output_shape=1)(encoding)
    outputs = layers.Dense(combined_params.n_labels, activation='softmax', name='softmax')(x)

    is_unknown = OpensetUnknownLayer(combined_params.openset_threshold, name='is_unknown')(outputs, score)

    combined_model = OpensetModel(inputs=[input_A, input_B, input_C],
                                  outputs=[outputs, is_unknown, encoding, score])

    tlmodel = combined_params.model_class()

    train_ds, valid_ds = tlmodel.create_dataset(dataset, combined_params, training=True, filter_by_label=True)

    experiment_name = "TransferLearning_Test"
    run_name = "run-02"
    experiment = mlflow.set_experiment(experiment_name)

    with mlflow.start_run(run_name=run_name, experiment_id=experiment.experiment_id):
        try:
            mlflow.log_params(vars(combined_params))  # log config file into mlflow
        except mlflow.exceptions.MlflowException:
            print('Failed to log parameters in MLflow due to the length limit')

        mlflow.tensorflow.autolog(log_datasets=False, log_models=False)

        # Pass combined_model to openset framework for training
        tlmodel.train(combined_model, train_ds, valid_ds, log_dir, combined_params)

        # due to ml has issue to save customized model, save by artifact instead
        with tempfile.TemporaryDirectory() as temp_dir:
            tlmodel.save(Path(temp_dir))
            mlflow.log_artifacts(temp_dir, run_name)

        # also log the config json file into artifact
        mlflow.log_artifact(params_c.name)
